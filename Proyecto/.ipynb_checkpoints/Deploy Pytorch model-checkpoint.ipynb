{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear una aplicación web de análisis de opiniones\n",
    "\n",
    "## Usando PyTorch y SageMaker\n",
    "\n",
    "Programa de aprendizaje profundo Nanodegree | Despliegue\n",
    "\n",
    "Ahora que tenemos una comprensión básica de cómo funciona SageMaker, intentaremos usarlo para construir un proyecto completo de principio a fin. Nuestro objetivo será tener una página web simple que un usuario pueda usar para ingresar una reseña de la película. La página web enviará la revisión a nuestro modelo implementado, que predecirá el sentimiento de la revisión ingresada.\n",
    "\n",
    "## Instrucciones\n",
    "Ya se le ha proporcionado un código de plantilla, y deberá implementar una funcionalidad adicional para completar con éxito este cuaderno. No necesitará modificar el código incluido más allá de lo solicitado. Las secciones que comienzan con 'TODO' en el encabezado indican que debe completar o implementar alguna parte dentro de ellas. Se proporcionarán instrucciones para cada sección y los detalles de la implementación están marcados en el bloque de código con un # TODO: ... comentario. ¡Asegúrese de leer las instrucciones cuidadosamente!\n",
    "\n",
    "Además de implementar el código, habrá preguntas para responder que se relacionan con la tarea y su implementación. Cada sección donde responderá una pregunta está precedida por un encabezado 'Pregunta:'. Lea atentamente cada pregunta y proporcione su respuesta debajo del encabezado 'Respuesta:' editando la celda Markdown.\n",
    "\n",
    "**Nota**: Las celdas de código y Markdown se pueden ejecutar usando el atajo de teclado Shift + Enter. Además, una celda se puede editar haciendo clic en ella (haciendo doble clic en las celdas Markdown) o presionando Entrar mientras está resaltada.\n",
    "\n",
    "## Bosquejo general\n",
    "Recuerde el esquema general de los proyectos de SageMaker utilizando una instancia de cuaderno.\n",
    "\n",
    "1. Descargue o recupere los datos.\n",
    "2. Procesar / preparar los datos.\n",
    "3. Suba los datos procesados a S3.\n",
    "4. Entrenar a un modelo elegido.\n",
    "5. Pruebe el modelo entrenado (generalmente usando un trabajo de transformación por lotes).\n",
    "6. Implemente el modelo entrenado.\n",
    "7. Use el modelo desplegado.\n",
    "Para este proyecto, seguirá los pasos del esquema general con algunas modificaciones.\n",
    "\n",
    "Primero, no probará el modelo en su propio paso. Todavía estará probando el modelo, sin embargo, lo hará implementando su modelo y luego utilizando el modelo implementado enviándole los datos de prueba. Una de las razones para hacerlo es para que pueda asegurarse de que su modelo implementado funcione correctamente antes de seguir adelante.\n",
    "\n",
    "Además, implementará y utilizará su modelo entrenado por segunda vez. En la segunda iteración, personalizará la forma en que se implementa su modelo entrenado al incluir parte de su propio código. Además, su modelo recién implementado se utilizará en la aplicación web de análisis de sentimientos.\n",
    "\n",
    "## Paso 1: descargando los datos\n",
    "Al igual que en el cuaderno XGBoost en SageMaker, utilizaremos el conjunto de datos de IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: preparación y procesamiento de los datos\n",
    "Además, como en el cuaderno XGBoost, haremos un procesamiento inicial de datos. Los primeros pasos son los mismos que en el ejemplo XGBoost. Para comenzar, leeremos cada una de las revisiones y las combinaremos en una sola estructura de entrada. Luego, dividiremos el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos leído los datos de entrenamiento y pruebas sin procesar del conjunto de datos descargado, combinaremos las revisiones positivas y negativas y mezclaremos los registros resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestros conjuntos de entrenamiento y prueba unificados y preparados, deberíamos hacer una verificación rápida y ver un ejemplo de los datos en los que se entrenará nuestro modelo. En general, es una buena idea, ya que le permite ver cómo cada uno de los pasos de procesamiento adicionales afecta las revisiones y también garantiza que los datos se hayan cargado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X[100])\n",
    "print(train_y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso para procesar las revisiones es asegurarse de que se eliminen las etiquetas html que aparezcan. Además, deseamos tokenizar nuestro aporte, de esa manera palabras como entretenido y entretenido se consideran lo mismo con respecto al análisis de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método review_to_words definido anteriormente utiliza BeautifulSoup para eliminar las etiquetas html que aparecen y utiliza el paquete nltk para simular las revisiones. Para verificar que sabemos cómo funciona todo, intente aplicar review_to_words a una de las revisiones del conjunto de capacitación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: aplicar review_to_words a una revisión (train_X [100] o cualquier otra revisión)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta**: Anteriormente mencionamos que el método review_to_words elimina el formato html y nos permite tokenizar las palabras encontradas en una revisión, por ejemplo, convertir entretenidos y entretenidos en entretener para que sean tratados como si fueran la misma palabra. ¿Qué más, si hay algo, le hace este método a la entrada?\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "El siguiente método aplica el método review_to_words a cada una de las revisiones en los conjuntos de datos de capacitación y prueba. Además, almacena en caché los resultados. Esto se debe a que realizar este paso de procesamiento puede llevar mucho tiempo. De esta manera, si no puede completar el cuaderno en la sesión actual, puede regresar sin necesidad de procesar los datos por segunda vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforma los datos\n",
    "En el cuaderno XGBoost transformamos los datos de su representación de palabras a una representación de características de bolsa de palabras. Para el modelo que vamos a construir en este cuaderno, construiremos una representación de características que es muy similar. Para comenzar, representaremos cada palabra como un número entero. Por supuesto, algunas de las palabras que aparecen en las revisiones ocurren con poca frecuencia y, por lo tanto, es probable que no contengan mucha información para el análisis de sentimientos. La forma en que trataremos este problema es que arreglaremos el tamaño de nuestro vocabulario de trabajo y solo incluiremos las palabras que aparecen con más frecuencia. Luego combinaremos todas las palabras poco frecuentes en una sola categoría y, en nuestro caso, la etiquetaremos como 1.\n",
    "\n",
    "Como utilizaremos una red neuronal recurrente, será conveniente que la duración de cada revisión sea la misma. Para hacer esto, fijaremos un tamaño para nuestras revisiones y luego rellenaremos revisiones cortas con la categoría 'sin palabra' (que etiquetaremos como 0) y truncaremos las revisiones largas.\n",
    "\n",
    "### (TODO) Crear un diccionario de palabras\n",
    "Para comenzar, necesitamos construir una forma de mapear las palabras que aparecen en las revisiones a enteros. Aquí fijamos el tamaño de nuestro vocabulario (incluidas las categorías 'sin palabra' e 'infrecuente') en 5000, pero es posible que desee cambiar esto para ver cómo afecta al modelo.\n",
    "\n",
    "> **TODO**: Complete la implementación para el método build_dict () a continuación. Tenga en cuenta que aunque vocab_size esté configurado en 5000, solo queremos construir un mapeo para las 4998 palabras que aparecen con más frecuencia. Esto se debe a que queremos reservar las etiquetas especiales 0 para 'sin palabra' y 1 para 'palabra infrecuente'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construye y devuelve un diccionario que asigna cada una\n",
    "    de las palabras que aparecen con más frecuencia a un número entero único\"\"\"\n",
    "    \n",
    "     #TODO: Determine con qué frecuencia aparece cada palabra en `data`.\n",
    "        #Tenga en cuenta que `data` es una lista de oraciones y que una oración es una lista de palabras.\n",
    "    word_count = {}\n",
    "    # Un diccionario que almacena las palabras que aparecen en las reseñas junto con la frecuencia con la que aparecen\n",
    "    \n",
    "     # TODO: ordena las palabras encontradas en `data` para que sorted_words [0] sea la palabra que aparece con más frecuencia y\n",
    "     # sorted_words [-1] es la palabra que aparece con menos frecuencia.\n",
    "    sorted_words = None\n",
    "    \n",
    "    word_dict = {} # Esto es lo que estamos construyendo, un diccionario que traduce palabras en enteros\n",
    "    \n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]):\n",
    "        word_dict[word] = idx + 2\n",
    "    # El -2 es para que ahorremos espacio para las etiquetas 'no word' 'infrequent'\n",
    "    return word_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta**: ¿Cuáles son las cinco palabras que aparecen con más frecuencia (tokenizadas) en el conjunto de entrenamiento? ¿Tiene sentido que estas palabras aparezcan con frecuencia en el conjunto de entrenamiento?\n",
    "\n",
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use este espacio para determinar las cinco palabras que aparecen con más frecuencia en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guarde word_dict\n",
    "Más adelante, cuando construyamos un punto final que procese una revisión enviada, necesitaremos usar el word_dict que hemos creado. Como tal, lo guardaremos en un archivo ahora para uso futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/pytorch' # Carpeta para guardar el archivo\n",
    "if not os.path.exists(data_dir): # Comprobar si la carpeta existe\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforma las reseñas\n",
    "Ahora que tenemos nuestro diccionario de palabras que nos permite transformar las palabras que aparecen en las revisiones en números enteros, es hora de usarlo y convertir nuestras revisiones a su representación de secuencia entera, asegurándose de rellenar o truncar a una longitud fija, que en nuestro caso es 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como una verificación rápida para asegurarse de que las cosas funcionan como se esperaba, verifique cómo se ve una de las revisiones en el conjunto de capacitación después de haber sido procesada. ¿Esto parece razonable? ¿Cuál es la duración de una revisión en el conjunto de capacitación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use esta celda para examinar una de las revisiones procesadas para asegurarse de que todo funcione según lo previsto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta**: En las celdas de arriba usamos los métodos preprocess_data y convert_and_pad_data para procesar tanto el conjunto de entrenamiento como el de pruebas. ¿Por qué o por qué no podría ser un problema?\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "## Paso 3: sube los datos a S3\n",
    "Al igual que en el cuaderno XGBoost, necesitaremos cargar el conjunto de datos de entrenamiento en S3 para que nuestro código de entrenamiento pueda acceder a él. Por ahora lo guardaremos localmente y lo subiremos a S3 más adelante.\n",
    "\n",
    "### Guarde el conjunto de datos de entrenamiento procesado localmente\n",
    "Es importante tener en cuenta el formato de los datos que estamos guardando, ya que necesitaremos saberlo cuando escribamos el código de capacitación. En nuestro caso, cada fila del conjunto de datos tiene la etiqueta del formulario, longitud, revisión [500] donde revisión [500] es una secuencia de 500 enteros que representan las palabras en la revisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subiendo los datos de entrenamiento\n",
    "A continuación, debemos cargar los datos de entrenamiento al depósito S3 predeterminado de SageMaker para que podamos proporcionar acceso a ellos mientras entrenamos nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA**: La celda de arriba carga todo el contenido de nuestro directorio de datos. Esto incluye el archivo word_dict.pkl. Esto es afortunado ya que lo necesitaremos más adelante cuando creamos un punto final que acepte una revisión arbitraria. Por ahora, solo tomaremos nota del hecho de que reside en el directorio de datos (y también en el grupo de entrenamiento S3) y que necesitaremos asegurarnos de que se guarde en el directorio del modelo.\n",
    "\n",
    "## Paso 4: Construye y entrena el modelo PyTorch\n",
    "En el cuaderno XGBoost discutimos qué es un modelo en el marco de SageMaker. En particular, un modelo comprende tres objetos.\n",
    "\n",
    "* Artefactos modelo,\n",
    "* Código de entrenamiento, y\n",
    "* Código de inferencia,\n",
    "cada uno de los cuales interactúa entre sí. En el ejemplo de XGBoost, utilizamos el código de capacitación e inferencia proporcionado por Amazon. Aquí todavía usaremos contenedores proporcionados por Amazon con el beneficio adicional de poder incluir nuestro propio código personalizado.\n",
    "\n",
    "Comenzaremos implementando nuestra propia red neuronal en PyTorch junto con un script de entrenamiento. Para los propósitos de este proyecto, hemos proporcionado el objeto modelo necesario en el archivo model.py, dentro de la carpeta del tren. Puede ver la implementación proporcionada ejecutando la celda a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize train/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La conclusión importante de la implementación proporcionada es que hay tres parámetros que podemos desear ajustar para mejorar el rendimiento de nuestro modelo. Estas son la dimensión de incrustación, la dimensión oculta y el tamaño del vocabulario. Es probable que queramos que estos parámetros sean configurables en el script de entrenamiento para que, si deseamos modificarlos, no necesitemos modificar el script en sí. Veremos cómo hacer esto más adelante. Para comenzar, escribiremos algunos de los códigos de capacitación en el cuaderno para que podamos diagnosticar más fácilmente cualquier problema que surja.\n",
    "\n",
    "Primero, cargaremos una pequeña porción del conjunto de datos de entrenamiento para usar como muestra. Trataría mucho tiempo intentar entrenar el modelo por completo en el portátil, ya que no tenemos acceso a una gpu y la instancia de cálculo que estamos utilizando no es particularmente poderosa. Sin embargo, podemos trabajar en una pequeña parte de los datos para tener una idea de cómo se comporta nuestro script de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Escribiendo el método de entrenamiento\n",
    "Luego necesitamos escribir el código de entrenamiento en sí. Esto debería ser muy similar a los métodos de entrenamiento que ha escrito antes para entrenar modelos PyTorch. Dejaremos algunos aspectos difíciles como guardar / cargar modelos y cargar parámetros hasta un poco más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
