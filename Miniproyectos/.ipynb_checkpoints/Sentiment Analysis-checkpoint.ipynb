{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de los sentimientos\n",
    "## Usando XGBoost en SageMaker\n",
    "\n",
    "Como nuestro primer ejemplo de uso del servicio SageMaker de Amazon, construiremos un modelo de árbol aleatorio para predecir el sentimiento de una reseña de película. Es posible que haya visto una versión de este ejemplo en una lección anterior, aunque se hubiera hecho usando el paquete sklearn. En cambio, utilizaremos el paquete XGBoost tal como nos lo proporciona Amazon.\n",
    "\n",
    "## Instrucciones\n",
    "Ya se le ha proporcionado un código de plantilla, y deberá implementar una funcionalidad adicional para completar con éxito este cuaderno. No necesitará modificar el código incluido más allá de lo solicitado. Las secciones que comienzan con 'TODO' en el encabezado indican que debe completar o implementar alguna parte dentro de ellas. Se proporcionarán instrucciones para cada sección y los detalles de la implementación están marcados en el bloque de código con un comentario #TODO: .... ¡Asegúrese de leer las instrucciones cuidadosamente!\n",
    "\n",
    "Además de implementar el código, es posible que tenga que responder preguntas relacionadas con la tarea y su implementación. Cada sección donde responderá una pregunta está precedida por un encabezado 'Pregunta:'. Lea atentamente cada pregunta y proporcione su respuesta debajo del encabezado 'Respuesta:' editando la celda Markdown.\n",
    "\n",
    "## Paso 1: descargando los datos\n",
    "El conjunto de datos que vamos a utilizar es muy popular entre los investigadores del procesamiento del lenguaje natural, generalmente denominado conjunto de datos IMDb. Consiste en reseñas de películas del sitio web imdb.com, cada una etiquetada como 'positiva', si el crítico disfrutó la película, o 'negativa' de lo contrario.\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011.\n",
    "\n",
    "Comenzamos usando un poco de magia de Jupyter Notebook para descargar y extraer el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir ../Data\n",
    "!wget .O ../Data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../Data/aclImdb_v1.tar.gz -C ../Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: preparar los datos\n",
    "Los datos que hemos descargado se dividen en varios archivos, cada uno de los cuales contiene una única revisión. Será mucho más fácil avanzar si combinamos estos archivos individuales en dos archivos grandes, uno para capacitación y otro para prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import shuffle\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_imbd_data(data_dir='../Data/aclImbd'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in file:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment] == len(labels[data_type][sentiment]),\n",
    "                       \"{}/{} El tamaño de los datos no coincide con el tamaño de los labels\".format(data_type, sentiment))\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = read_imbd_data()\n",
    "print('IMBD reviews: train = {} pos / {} neg, test = {} pos / {} neg'.format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Preparar el conjunto de entrenamiento y prueba para las criticas de las peliculas de IMDb.\"\"\"\n",
    "    \n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_imdb_date(data, labels)\n",
    "print(\"IMDb reviews (combinadas): train = {}, test = {}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Procesando los datos\n",
    "Ahora que tenemos nuestros conjuntos de datos de entrenamiento y prueba combinados y listos para usar, necesitamos comenzar a procesar los datos sin procesar en algo que nuestro algoritmo de aprendizaje automático pueda usar. Para comenzar, eliminamos cualquier formato html que pueda aparecer en las revisiones y realizamos un procesamiento estándar del lenguaje natural para homogeneizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, 'html.parser').get_text()\n",
    "    text = re.sub(r\"[â-zA-Z0-9]\", \"\", text.lower())\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")\n",
    "os.makedirs(cache_dir, exis_ok=True)\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data_pkl\"):\n",
    "    \"\"\"Convierte cada review en palabras; lee desde el archivo caché si esta disponible\"\"\"\n",
    "    \n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    \n",
    "    if cache_data is None:\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "       \n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer características de la bolsa de palabras\n",
    "Para el modelo que implementaremos, en lugar de utilizar las revisiones directamente, vamos a transformar cada revisión en una representación característica de la Bolsa de palabras. Tenga en cuenta que 'en la naturaleza' solo tendremos acceso al conjunto de entrenamiento para que nuestro transformador solo pueda usar el conjunto de entrenamiento para construir una representación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
    "                preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, vocabulary = extract_BoW_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Clasificación usando XGBoost\n",
    "Ahora que hemos creado la representación de características de nuestros datos de entrenamiento (y pruebas), es hora de comenzar a configurar y usar el clasificador XGBoost provisto por SageMaker.\n",
    "\n",
    "### (TODO) Escribiendo el conjunto de datos\n",
    "El clasificador XGBoost que utilizaremos requiere que el conjunto de datos se escriba en un archivo y se almacene con Amazon S3. Para hacer esto, comenzaremos dividiendo el conjunto de datos de entrenamiento en dos partes, los datos con los que entrenaremos al modelo y un conjunto de validación. Luego, escribiremos esos conjuntos de datos en un archivo y subiremos los archivos a S3. Además, escribiremos la entrada del conjunto de prueba en un archivo y subiremos el archivo a S3. Esto es para que podamos usar la funcionalidad de Transformación por lotes de SageMakers para probar nuestro modelo una vez que lo hayamos ajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Split the train_X and train_y arrays into the DataFrames val_X, train_X and val_y, train_y. Make sure that\n",
    "#       val_X and val_y contain 10 000 entires while train_X and train_y contain the remaining 15 000 entries.\n",
    "#val_X = pd.DataFrame(None)\n",
    "#train_X = pd.DataFrame(None)\n",
    "\n",
    "#val_y = pd.DataFrame(None)\n",
    "#train_y = pd.DataFrame(None)\n",
    "\n",
    "# Solution:\n",
    "# Earlier we shuffled the training dataset so to make things simple we can just assign\n",
    "# the first 10 000 reviews to the validation set and use the remaining reviews for training.\n",
    "val_X = pd.DataFrame(train_X[:10000])\n",
    "train_X = pd.DataFrame(train_X[10000:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:10000])\n",
    "train_y = pd.DataFrame(train_y[10000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La documentación para el algoritmo XGBoost en SageMaker requiere que los conjuntos de datos guardados no contengan encabezados o índices y que para los datos de capacitación y validación, la etiqueta debe aparecer primero para cada muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Data/xgboost'\n",
    "if not in os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, save the test data to test.csv in the data_dir directory. Note that we do not save the associated ground truth\n",
    "# labels, instead we will use them later to compare with our model output.\n",
    "\n",
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "# TODO: Save the training and validation data to train.csv and validation.csv in the data_dir directory.\n",
    "#       Make sure that the files you create are in the correct format.\n",
    "\n",
    "# Solution:\n",
    "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save a bit of memory we can set text_X, train_X, val_X, train_y and val_y to None.\n",
    "\n",
    "test_X = train_X = val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Carga de archivos de entrenamiento/validación a S3\n",
    "El servicio S3 de Amazon nos permite almacenar archivos a los que pueden acceder tanto los modelos de entrenamiento incorporados como el modelo XGBoost que usaremos, como los modelos personalizados como el que veremos un poco más adelante.\n",
    "\n",
    "Para esta y la mayoría de las otras tareas que realizaremos con SageMaker, hay dos métodos que podríamos usar. El primero es utilizar la funcionalidad de bajo nivel de SageMaker que requiere conocer cada uno de los objetos involucrados en el entorno de SageMaker. El segundo es utilizar la funcionalidad de alto nivel en la que se han hecho ciertas elecciones en nombre del usuario. El enfoque de bajo nivel se beneficia al permitir al usuario una gran flexibilidad mientras que el enfoque de alto nivel hace que el desarrollo sea mucho más rápido. Para nuestros propósitos, optaremos por utilizar el enfoque de alto nivel, aunque usar el enfoque de bajo nivel es ciertamente una opción.\n",
    "\n",
    "Recuerde el método **upload_data()** que es un miembro del objeto que representa nuestra sesión actual de SageMaker. Lo que hace este método es cargar los datos en el depósito predeterminado (que se crea si no existe) en la ruta descrita por la variable **key_prefix**. Para ver esto por sí mismo, una vez que haya cargado los archivos de datos, vaya a la consola S3 y mire para ver dónde se han cargado los archivos.\n",
    "\n",
    "Para obtener recursos adicionales, consulte la documentación de la API de SageMaker y, además, la Guía del desarrollador de SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "prefix = 'sentiment-xgboost'\n",
    "\n",
    "# TODO: Upload the test.csv, train.csv and validation.csv files which are contained in data_dir to S3 using sess.upload_data().\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Creando el modelo XGBoost\n",
    "Ahora que los datos se han cargado, es hora de crear el modelo XGBoost. Para empezar, necesitamos hacer algo de configuración. En este punto, vale la pena discutir qué es un modelo en SageMaker. Es más fácil pensar en un modelo que comprenda tres objetos diferentes en el ecosistema SageMaker, que interactúan entre sí.\n",
    "\n",
    "* Artefactos modelo\n",
    "* Código de entrenamiento (contenedor)\n",
    "* Código de inferencia (contenedor)\n",
    "\n",
    "Los artefactos del modelo son lo que usted podría considerar como el modelo en sí. Por ejemplo, si estuviera construyendo una red neuronal, los artefactos del modelo serían los pesos de las diversas capas. En nuestro caso, para un modelo XGBoost, los artefactos son los árboles reales que se crean durante el entrenamiento.\n",
    "\n",
    "Los otros dos objetos, el código de entrenamiento y el código de inferencia se utilizan para manipular los artefactos de entrenamiento. Más precisamente, el código de entrenamiento usa los datos de entrenamiento que se proporcionan y crea los artefactos del modelo, mientras que el código de inferencia usa los artefactos del modelo para hacer predicciones sobre nuevos datos.\n",
    "\n",
    "La forma en que SageMaker ejecuta el código de capacitación e inferencia es mediante el uso de contenedores Docker. Por ahora, piense en un contenedor como una forma de empaquetar código para que las dependencias no sean un problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Our current execution role is require when creating the model as the training\n",
    "# and inference code will need to access the model artifacts.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to retrieve the location of the container which is provided by Amazon for using XGBoost.\n",
    "# As a matter of convenience, the training and inference code both use the same container.\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a SageMaker estimator using the container location determined in the previous cell.\n",
    "#       It is recommended that you use a single training instance of type ml.m4.xlarge. It is also\n",
    "#       recommended that you use 's3://{}/{}/output'.format(session.default_bucket(), prefix) as the\n",
    "#       output path.\n",
    "\n",
    "# Solution:\n",
    "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "# TODO: Set the XGBoost hyperparameters in the xgb object. Don't forget that in this case we have a binary\n",
    "#       label so we should be using the 'binary:logistic' objective.\n",
    "\n",
    "# Solution:\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustar el modelo XGBoost\n",
    "Ahora que nuestro modelo ha sido configurado, simplemente necesitamos adjuntar los conjuntos de datos de capacitación y validación y luego pedirle a SageMaker que configure el cálculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Prueba del modelo\n",
    "Ahora que nos hemos adaptado a nuestro modelo XGBoost, es hora de ver qué tan bien funciona. Para hacerlo, utilizaremos la funcionalidad de transformación por lotes de SageMakers. Batch Transform es una forma conveniente de realizar inferencia en un gran conjunto de datos de una manera que no es en tiempo real. Es decir, no necesariamente necesitamos usar los resultados de nuestro modelo de inmediato y en su lugar podemos realizar inferencias en una gran cantidad de muestras. Un ejemplo de esto en la industria podría ser realizar un informe de fin de mes. Este método de inferencia también puede sernos útil, ya que significa realizar inferencia en todo nuestro conjunto de pruebas.\n",
    "\n",
    "Para realizar una transformación por lotes, primero debemos crear un transformador a partir de nuestro objeto estimador entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a transformer object from the trained model. Using an instance count of 1 and an instance type of ml.m4.xlarge\n",
    "#       should be more than enough.\n",
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, realizamos el trabajo de transformación. Al hacerlo, debemos asegurarnos de especificar el tipo de datos que estamos enviando para que se serialicen correctamente en segundo plano. En nuestro caso, proporcionamos a nuestro modelo datos csv, por lo que especificamos text/csv. Además, si los datos de prueba que proporcionamos son demasiado grandes para procesarlos a la vez, entonces debemos especificar cómo se debe dividir el archivo de datos. Como cada línea es una entrada única en nuestro conjunto de datos, le decimos a SageMaker que puede dividir la entrada en cada línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Start the transform job. Make sure to specify the content type and the split type of the test data.\n",
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualmente el trabajo de transformación se está ejecutando pero lo está haciendo en segundo plano. Como deseamos esperar hasta que se complete el trabajo de transformación y nos gustaría un poco de retroalimentación, podemos ejecutar el método **wait()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el trabajo de transformación se ha ejecutado y el resultado, el sentimiento estimado de cada revisión, se ha guardado en S3. Como preferimos trabajar en este archivo localmente, podemos realizar un poco de magia de cuaderno para copiar el archivo en el data_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El último paso ahora es leer la salida de nuestro modelo, convertir la salida a algo un poco más utilizable, en este caso queremos que el sentimiento sea 1 (positivo) o 0 (negativo), y luego compararlo con el suelo Etiquetas de verdad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcional: limpiar\n",
    "La instancia de notebook predeterminada en SageMaker no tiene mucho espacio en disco disponible. A medida que continúe completando y ejecutando cuadernos, eventualmente llenará este espacio en disco, lo que generará errores que pueden ser difíciles de diagnosticar. Una vez que haya terminado de usar un cuaderno, es una buena idea eliminar los archivos que creó en el camino. Por supuesto, puede hacerlo desde la terminal o desde el hub del portátil si lo desea. La celda a continuación contiene algunos comandos para limpiar los archivos creados desde el cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will remove all of the files contained in the data_dir directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# And then we delete the directory itself\n",
    "!rmdir $data_dir\n",
    "\n",
    "# Similarly we will remove the files in the cache_dir directory and the directory itself\n",
    "!rm $cache_dir/*\n",
    "!rmdir $cache_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
